{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin link prediction\n",
    "\n",
    "Material for Lecture 4 of the Risk Analytics workshop at the University of Ljubljana.\n",
    "\n",
    "Please note: this implementation has not been optimized and is in need of refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import requests\n",
    "\n",
    "from itertools import combinations_with_replacement as cwr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "\n",
    "#from node2vec import node2vec as n2v\n",
    "#from node2vec import node2vec as n2v\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "#reload(n2v)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_embeddings(walks, dimensions, window_size, workers, n_iter=1):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    '''\n",
    "    \n",
    "    # Python 2 or 3\n",
    "    if sys.version_info.major < 3:\n",
    "        walks = [map(str, walk) for walk in walks]\n",
    "    else: # Python 3\n",
    "        walks = [list(map(str,walk)) for walk in walks]\n",
    "        \n",
    "    model = Word2Vec(walks, size=dimensions, window=window_size, min_count=0, sg=1, workers=workers, iter=n_iter)\n",
    "    #model.save_word2vec_format(args.output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def common_neighbor(pair, G):\n",
    "    com_nbr = len(list(nx.common_neighbors(G, pair[0], pair[1])))\n",
    "\n",
    "    return com_nbr\n",
    "\n",
    "\n",
    "def hadamard(pair, model):\n",
    "    hadamard = np.dot(model.wv[pair[0]], model.wv[pair[1]])\n",
    "    \n",
    "    return hadamard\n",
    "\n",
    "# Modified version of https://github.com/aditya-grover/node2vec/blob/master/src/node2vec.py\n",
    "import random\n",
    "\n",
    "class n2vGraph():\n",
    "    def __init__(self, nx_G, is_directed, p, q):\n",
    "        self.G = nx_G\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        '''\n",
    "        Simulate a random walk starting from start node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = sorted(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
    "                        alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length):\n",
    "        '''\n",
    "        Repeatedly simulate random walks from each node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        walks = []\n",
    "        nodes = list(G.nodes())\n",
    "        print('Walk iteration:')\n",
    "        for walk_iter in range(num_walks):\n",
    "            print(str(walk_iter+1), '/', str(num_walks))\n",
    "            random.shuffle(nodes)\n",
    "            for node in nodes:\n",
    "                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def get_alias_edge(self, src, dst):\n",
    "        '''\n",
    "        Get the alias edge setup lists for a given edge.\n",
    "        '''\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "\n",
    "        # PL mod: differentiate between graphs types (multi or not)\n",
    "        # PL TODO: make random walk also along random connecting edgnes, not just 0th\n",
    "        if isinstance(G, nx.multigraph.MultiGraph):\n",
    "            for dst_nbr in sorted(G.neighbors(dst)):\n",
    "                if dst_nbr == src:\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one \n",
    "                    unnormalized_probs.append(G[dst][dst_nbr][0]['weight']/p)\n",
    "                elif G.has_edge(dst_nbr, src):\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one\n",
    "                    unnormalized_probs.append(G[dst][dst_nbr][0]['weight'])\n",
    "                else:\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one\n",
    "                    unnormalized_probs.append(G[dst][dst_nbr][0]['weight']/q)\n",
    "        else: # Is not multigraph:\n",
    "            for dst_nbr in sorted(G.neighbors(dst)):\n",
    "                if dst_nbr == src:\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one \n",
    "                    unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "                elif G.has_edge(dst_nbr, src):\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one\n",
    "                    unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "                else:\n",
    "                    # Multigraph TODO: can be more than one edge, 0 only the 1st one\n",
    "                    unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return alias_setup(normalized_probs)\n",
    "\n",
    "    def preprocess_transition_probs(self):\n",
    "        '''\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        '''\n",
    "        G = self.G\n",
    "        is_directed = self.is_directed\n",
    "\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr][0]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def alias_setup(probs):\n",
    "    '''\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    '''\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    '''\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    '''\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "\n",
    "def learn_embeddings(walks, dimensions, window_size, workers, n_iter=1):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    Transplanted from main.py and make Python 3 compatible\n",
    "    '''\n",
    "    \n",
    "    # Python 2 or 3\n",
    "    if sys.version_info.major < 3:\n",
    "        walks = [map(str, walk) for walk in walks]\n",
    "    else: # Python 3\n",
    "        walks = [list(map(str,walk)) for walk in walks]\n",
    "        \n",
    "    model = Word2Vec(walks, size=dimensions, window=window_size, min_count=0, sg=1, workers=workers, iter=n_iter)\n",
    "    #model.save_word2vec_format(args.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do link prediction on the bitcoin transaction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_me = False\n",
    "graph_df = pd.read_csv(\"../data/btc_df.csv\")\n",
    "graph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out the mining transactions\n",
    "graph_df = graph_df.loc[~(graph_df.addr_from == 'mining'), :]\n",
    "graph_df = graph_df.rename(columns={'value': 'weight'})\n",
    "\n",
    "run_list = []\n",
    "for height_max in range(55000, 65000, 1000):\n",
    "    run_dict = {}\n",
    "    print(height_max)\n",
    "    height_train = int(height_max*0.90)\n",
    "\n",
    "    run_dict['height_max'] = height_max\n",
    "    run_dict['height_train'] = height_train\n",
    "    graph_df = graph_df.loc[graph_df.height < height_max, :]\n",
    "    graph_train = graph_df.loc[graph_df.height < height_train, :]\n",
    "    graph_test = graph_df.loc[graph_df.height >= height_train, :]\n",
    "    graph_df.head()\n",
    "\n",
    "    # Create edge train and test sets\n",
    "    edge_df = graph_df[['addr_from', 'addr_to']]#, 'weight']]\n",
    "    edge_df['data'] = graph_df[['weight', 'height']].to_dict(orient='records')\n",
    "    #edge_df['data'] = graph_df[['weight']].to_dict(orient='records')\n",
    "    edge_list = edge_df.values.tolist()\n",
    "\n",
    "    edge_train = graph_train[['addr_from', 'addr_to']]#, 'weight']]\n",
    "    edge_train['data'] = graph_train[['weight', 'height']].to_dict(orient='records')\n",
    "    #edge_df['data'] = graph_df[['weight']].to_dict(orient='records')\n",
    "    edge_train_list = edge_train.values.tolist()\n",
    "\n",
    "    edge_test = graph_test[['addr_from', 'addr_to']]#, 'weight']]\n",
    "    edge_test['data'] = graph_test[['weight', 'height']].to_dict(orient='records')\n",
    "    #edge_df['data'] = graph_df[['weight']].to_dict(orient='records')\n",
    "    edge_test_list = edge_test.values.tolist()\n",
    "\n",
    "\n",
    "    # Create graphs, full, train and test\n",
    "    G = nx.MultiDiGraph(edge_list)\n",
    "    G_train = nx.MultiDiGraph(edge_train_list)\n",
    "    G_test = nx.MultiDiGraph(edge_test_list)\n",
    "    #nx.draw_networkx(G, with_labels=False, node_size = 80)\n",
    "\n",
    "    print(\"Number of nodes: {0}\\nNumber of edges: {1}\".format(G.number_of_nodes(), G.number_of_edges()))\n",
    "    print(\"Number of train nodes: {0}\\nNumber of edges: {1}\".format(G_train.number_of_nodes(), G_train.number_of_edges()))\n",
    "    print(\"Number of test nodes: {0}\\nNumber of edges: {1}\".format(G_test.number_of_nodes(), G_test.number_of_edges()))\n",
    "\n",
    "    # Keep only nodes with valency at least 3\n",
    "    nodes_ge3 = [n for n in G.nodes() if G.in_degree(n) + G.out_degree(n) >=3]\n",
    "    G = G.subgraph(nodes_ge3)\n",
    "    #degs_in = G.in_degree()\n",
    "    #print(degs_in, '\\n')\n",
    "    #degs_out = G.out_degree()\n",
    "\n",
    "    nodes_train_ge3 = [n for n in G_train.nodes() if G_train.in_degree(n) + G_train.out_degree(n) >=3]\n",
    "    G_train = G_train.subgraph(nodes_train_ge3)\n",
    "    #degs_train_in = G_train.in_degree()\n",
    "    #print(degs_in, '\\n')\n",
    "    #degs_train_out = G_train.out_degree()\n",
    "\n",
    "    nodes_test_ge3 = [n for n in G_test.nodes() if G_test.in_degree(n) + G_test.out_degree(n) >=3]\n",
    "    G_test = G_test.subgraph(nodes_test_ge3)\n",
    "\n",
    "\n",
    "    G_und = G.to_undirected()\n",
    "    G_train_und = G_train.to_undirected()\n",
    "    G_test_und = G_test.to_undirected()\n",
    "    # Assign transaction amount as weight\n",
    "    print(\"Number of nodes: {0}\\nNumber of edges: {1}\".format(G.number_of_nodes(), G.number_of_edges()))\n",
    "    print(\"Number of nodes: {0}\\nNumber of edges: {1}\".format(G_train.number_of_nodes(), G_train.number_of_edges()))\n",
    "    print(\"Number of nodes: {0}\\nNumber of edges: {1}\".format(G_test.number_of_nodes(), G_test.number_of_edges()))\n",
    "\n",
    "    run_dict['n_train_nodes'] = G_train.number_of_nodes()\n",
    "    run_dict['n_train_edges'] = G_train.number_of_edges()\n",
    "    run_dict['n_test_nodes'] = G_test.number_of_nodes()\n",
    "    run_dict['n_test_edges'] = G_test.number_of_edges()\n",
    "\n",
    "    # node2vec parameters\n",
    "    p, q = 1,3\n",
    "\n",
    "    G_n2v = n2vGraph(G_train, True, p, q)\n",
    "    G_n2v.preprocess_transition_probs()\n",
    "\n",
    "\n",
    "    walks = G_n2v.simulate_walks(20, 10)\n",
    "\n",
    "    # word2vec params\n",
    "    dimensions = 100\n",
    "    window_size = 5\n",
    "    workers =4\n",
    "    n_iter = 10\n",
    "    model = learn_embeddings(walks, dimensions, window_size, workers, n_iter=n_iter)\n",
    "\n",
    "    # Define train and test node pairs and edges\n",
    "    pairs_train = cwr(G_train.nodes(), 2)\n",
    "    pairs_train_list = list(pairs_train)\n",
    "\n",
    "    pairs_test = cwr(G_test.nodes(), 2)\n",
    "    pairs_test_list = list(set(pairs_test).intersection(set(pairs_train_list)))\n",
    "\n",
    "    e_train = G_train.edges()\n",
    "    e_test = G_test.edges()\n",
    "\n",
    "    # Create dataframe for training and testing\n",
    "    %time edge_mask = [pair in e_train or (pair[1], pair[0]) in e_train for pair in pairs_train_list]\n",
    "    train_df = pd.DataFrame({'node_pair': pairs_train_list})\n",
    "    train_df['edge_mask'] = edge_mask\n",
    "    %time train_df['com_nbr'] = [common_neighbor(pair, G_train_und) for pair in pairs_train_list]\n",
    "    %time train_df['hadamard'] = [np.dot(model.wv[pair[0]], model.wv[pair[1]]) for pair in pairs_train_list]\n",
    "\n",
    "    print(train_df.shape)\n",
    "    print(sum(train_df['edge_mask']))\n",
    "    print(auc(train_df['edge_mask'], train_df['com_nbr']))\n",
    "    print(auc(train_df['edge_mask'], train_df['hadamard']))\n",
    "    run_dict['auc_common_neighbors_train'] = auc(train_df['edge_mask'], train_df['com_nbr'])\n",
    "    run_dict['auc_n2v_train'] = auc(train_df['edge_mask'], train_df['hadamard'])\n",
    "    \n",
    "    %time edge_mask = [pair in e_test or (pair[1], pair[0]) in e_test for pair in pairs_test_list]\n",
    "    test_df = pd.DataFrame({'node_pair': pairs_test_list})\n",
    "    test_df['edge_mask'] = edge_mask\n",
    "    %time test_df['com_nbr'] = [common_neighbor(pair, G_test_und) for pair in pairs_test_list]\n",
    "    %time test_df['hadamard'] = [np.dot(model.wv[pair[0]], model.wv[pair[1]]) for pair in pairs_test_list]\n",
    "\n",
    "    print(auc(test_df['edge_mask'], test_df['com_nbr']))\n",
    "    print(auc(test_df['edge_mask'], test_df['hadamard']))\n",
    "    run_dict['auc_common_neighbors_test'] = auc(test_df['edge_mask'], test_df['com_nbr'])\n",
    "    run_dict['auc_n2v_test'] = auc(test_df['edge_mask'], test_df['hadamard'])\n",
    "    \n",
    "    run_list.append(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_df = pd.DataFrame(run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run = pd.Series(run_dict)\n",
    "run_df.set_index('height_max', inplace=True)\n",
    "if save_me:\n",
    "    run_df.to_csv('../results/n2v_runs.csv', header=False)\n",
    "run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = \"../slides/figures\"\n",
    "plt.figure();\n",
    "ax = run_df[['auc_common_neighbors_test', 'auc_n2v_test']].plot(figsize=(15, 10), linewidth=3.0);\n",
    "ax.set_xlabel(\"Block height\", fontsize=35)\n",
    "plt.legend(loc=3, prop={'size': 30})\n",
    "plt.tick_params(axis='both', which='major', labelsize=25)\n",
    "if save_me:\n",
    "    plt.savefig(os.path.join(fig_dir, \"cn_vs_n2v.png\"), format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems: Link Prediction\n",
    "\n",
    "### Link prediction heuristics\n",
    "\n",
    "1. Implement the Jaccard coefficient and compare its performance to Common Neighbors and node2vec\n",
    "2. Same for Adamic-Adir\n",
    "3. Same for Preferential Attachment\n",
    "\n",
    "### Refactor the above for loop over `height_max`\n",
    "\n",
    "Possible improvements:\n",
    "\n",
    "1. `*` Select sections of code to refactor as functions to both encapsulate and make the code more readable\n",
    "2. `**` Use dask or some other package to use parallelize the for loop\n",
    "3. `**` Refactor the split into train and test graphs to ensure a more consistent split between the number of training and testing nodes\n",
    "\n",
    "### Improve node2vec link prediction\n",
    "\n",
    "1. `*` Change the parameters of node2vec (e.g. embedding dimension, number and length of walks, p, q) to try to increase the prediction power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Mini-mining\n",
    "\n",
    "Find the smallest non-negative seed for np.random.uniform in Python (or runif in R) such that\n",
    "\n",
    "1. The first random number starts with one zero\n",
    "2. The first random number starts with two zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
