{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Feature Learning\n",
    "Examples from lecture 3, Risk Analytics Workshop at the University of Ljubljana, 2-3 November 2017\n",
    "\n",
    "Resources:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* https://rare-technologies.com/word2vec-tutorial/\n",
    "* https://www.tensorflow.org/tutorials/word2vec\n",
    "* https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "* https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb or https://github.com/munichpavel/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "#from itertools import combinations\n",
    "\n",
    "#import cython\n",
    "#import smart_open\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#nltk.download()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example: Ne Joci Peter\n",
    "\n",
    "## Mini example\n",
    "\"A si ti\" vs \"Ali si ti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>si</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2\n",
       "a    1.0  0.0  0.0\n",
       "ali  0.0  1.0  0.0\n",
       "si   0.0  0.0  1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_me = False\n",
    "njp_mini = ['a', 'ali', 'si']\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "vocab_labels = le.fit_transform(njp_mini)\n",
    "print(\"Labels: {0}\".format(vocab_labels))\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "mini = pd.DataFrame(enc.fit_transform([[label] for label in vocab_labels]).toarray(), index=njp_mini)\n",
    "#ohp = ohp.reindex(['a', 'ali', 'si', 'ti', 'tud', 'tudi', 'not', 'noter', 'padu', 'padel'])\n",
    "if save_me:\n",
    "    mini.to_csv(\"../slides/slide_data/mini.csv\")\n",
    "\n",
    "mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less mini example\n",
    "\"A si ti tud not padu\" vs \"Ali si ti tudi noter padel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NJP vocabulary: ['ali', 'noter', 'tud', 'ti', 'padu', 'not', 'tudi', 'padel', 'si', 'a']\n"
     ]
    }
   ],
   "source": [
    "njp_contexts = [\"A si ti tud not padu\", \"Ali si tudi noter padel\"]\n",
    "# Split words by spaces\n",
    "njp_splits = [c.split(\" \") for c in njp_contexts]\n",
    "# Flatten to get vocabulary\n",
    "njp_vocab = list(set([word.lower() for context in njp_splits for word in context]))\n",
    "print(\"NJP vocabulary: {0}\".format(njp_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform label and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1 3 8 7 5 2 9 4 6 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>si</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tud</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tudi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noter</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>padu</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>padel</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8    9\n",
       "a      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "ali    0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "si     0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "ti     0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "tud    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       "tudi   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "not    0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "noter  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "padu   0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "padel  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "vocab_labels = le.fit_transform(njp_vocab)\n",
    "print(\"Labels: {0}\".format(vocab_labels))\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "ohp = pd.DataFrame(enc.fit_transform([[label] for label in vocab_labels]).toarray(), index=njp_vocab)\n",
    "ohp = ohp.reindex(['a', 'ali', 'si', 'ti', 'tud', 'tudi', 'not', 'noter', 'padu', 'padel'])\n",
    "if save_me:\n",
    "    ohp.to_csv(\"../slides/slide_data/ohp.csv\")\n",
    "ohp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry of one hot encoded NJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner product <a, ali>: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Inner product <a, ali>: {0}\".format(np.dot(ohp.loc['a', :], ohp.loc['ali', :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words, NJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>ali</th>\n",
       "      <th>si</th>\n",
       "      <th>ti</th>\n",
       "      <th>tud</th>\n",
       "      <th>tudi</th>\n",
       "      <th>not</th>\n",
       "      <th>noter</th>\n",
       "      <th>padu</th>\n",
       "      <th>padel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>peter</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pravilno</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a  ali  si  ti  tud  tudi  not  noter  padu  padel\n",
       "peter     0    0   1   1    1     0    1      0     1      0\n",
       "pravilno  0    1   1   1    0     1    0      1     0      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(vocabulary = njp_vocab)\n",
    "# Note that the count vectorizer strips the question marks\n",
    "njp = ['A si ti tud not padu?', 'Ali si ti tudi noter padel?']\n",
    "X = vectorizer.fit_transform(njp)\n",
    "bow_njp = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names(), index=['peter', 'pravilno'])\n",
    "bow_njp = bow_njp[['a', 'ali', 'si', 'ti', 'tud', 'tudi', 'not', 'noter', 'padu', 'padel']]\n",
    "if save_me:\n",
    "    bow_njp.to_csv('../slides/slide_data/bow_njp.csv')\n",
    "bow_njp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "For background on tf-idf, see http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html or https://en.wikipedia.org/wiki/Tf–idf\n",
    "* Label and one hot encode the vocabulary of \"ce bi cebula ce ni imela, bi cebula bula bla\"\n",
    "* Calculate the BoW count vectors for the two sentences: [\"ce bi cebula ce ni imela\", \"bi cebula bula bla\"] using your encoding from the previous problem.\n",
    "* Calculate the tf-idf vectors of the two sentences above. Verify two entries of your vectors by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis with Bag of Words\n",
    "\n",
    "- 12.5k postive and 12.5k negative reviews in train set\n",
    "- 12.5k postive and 12.5k negative reviews in test set\n",
    "- Available at http://ai.stanford.edu/~amaas//data/sentiment/\n",
    "- Citation: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts, *Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics* (2011)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First download the IMDB data from http://ai.stanford.edu/~amaas//data/sentiment/\n",
    "# and put it in the data directory of your repository.\n",
    "# Alternatively, you could use the code from \n",
    "# https://github.com/munichpavel/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "# but then much of the fun (i.e. text cleansing) is done for you\n",
    "\n",
    "# Read in imdb review\n",
    "imdb_dir = os.path.join(\"../data\", \"aclImdb\")\n",
    "train_dir = os.path.join(imdb_dir, \"train\")\n",
    "test_dir = os.path.join(imdb_dir, \"test\")\n",
    "\n",
    "train_pos_dir = os.path.join(train_dir, 'pos')\n",
    "train_pos_files = glob(os.path.join(train_pos_dir, \"*.txt\"))\n",
    "\n",
    "train_neg_dir = os.path.join(train_dir, 'neg')\n",
    "train_neg_files = glob(os.path.join(train_neg_dir, \"*.txt\"))\n",
    "\n",
    "train_unsup_dir = os.path.join(train_dir, 'unsup')\n",
    "\n",
    "test_pos_dir = os.path.join(test_dir, 'pos')\n",
    "test_pos_files = glob(os.path.join(test_pos_dir, \"*.txt\"))\n",
    "\n",
    "test_neg_dir = os.path.join(test_dir, 'neg')\n",
    "test_neg_files = glob(os.path.join(test_neg_dir, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "train_file = train_pos_files[i]\n",
    "train_pos = []\n",
    "with open(train_file) as f:\n",
    "        for line in f:\n",
    "            train_pos.append(line)\n",
    "train_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize vectorizer to read from review files\n",
    "Note that the token pattern argument given below is the default one, and could have been omitted. The default is\n",
    "\n",
    "`token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'`. \n",
    "\n",
    "You might want to understand what this means for one of the tutorial problems :)\n",
    "\n",
    "Some resources:\n",
    "* https://docs.python.org/3/howto/regex.html\n",
    "* https://stackoverflow.com/questions/29689516/find-words-of-length-4-using-regular-expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on a small sample\n",
    "Note that punctuation has been removed and all words are converted to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 87\n"
     ]
    }
   ],
   "source": [
    "# Change n_sample to more reviews\n",
    "n_sample = 1\n",
    "vectorizer = CountVectorizer(input='filename', decode_error='ignore', strip_accents='unicode',\n",
    "                            token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "vectors = vectorizer.fit_transform(train_pos_files[:n_sample])\n",
    "vocab_default = vectorizer.get_feature_names()\n",
    "print(\"Number of words in vocabulary: {}\".format(len(vocab_default)))\n",
    "#vocab_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want numbers in the review, so we specify the token pattern to only get letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 86\n",
      "Words omitted: {'35'}\n"
     ]
    }
   ],
   "source": [
    "token_pattern = '(?u)\\\\b[a-z][a-z]+\\\\b'\n",
    "#token_pattern = '(?u)[A-Za-z]*\\\\b\\\\w\\\\w+'\n",
    "vectorizer = CountVectorizer(input='filename', decode_error='ignore', strip_accents='unicode', \n",
    "                             token_pattern=token_pattern)\n",
    "vectors = vectorizer.fit_transform(train_pos_files[:n_sample])\n",
    "vocab_letters_only = vectorizer.get_feature_names()\n",
    "print(\"Number of words in vocabulary: {}\".format(len(vocab_letters_only)))\n",
    "#vocab_letters_only\n",
    "# See what words have been omitted\n",
    "print(\"Words omitted: {0}\".format(set(vocab_default).difference(set(vocab_letters_only))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer on all training reviews for BoW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', decode_error='ignore', strip_accents='unicode',\n",
    "                            token_pattern='(?u)\\\\b[a-z][a-z]+\\\\b')\n",
    "bow_train = vectorizer.fit_transform(train_pos_files + train_neg_files)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# See word counts per review\n",
    "#bow_train.toarray().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create target vectors: 1 for positive review, 0 for negative and train logistic regression classifier\n",
    "y_train = np.concatenate([np.repeat(1, len(train_pos_files)), np.repeat(0, len(train_neg_files))])\n",
    "clf = LogisticRegression().fit(bow_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_test = vectorizer.transform(test_pos_files + test_neg_files)\n",
    "y_test = np.concatenate([np.repeat(1, len(test_pos_files)), np.repeat(0, len(test_neg_files))])\n",
    "y_pred = clf.predict(bow_test)\n",
    "y_proba = clf.predict_proba(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at some of these reviews\n",
    "test_pos = []\n",
    "for file in test_pos_files:\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            test_pos.append(line)\n",
    "            \n",
    "test_neg = []\n",
    "for file in test_neg_files:\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            test_neg.append(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at a few reviews to see how BoW does on reviews it has not yet seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number: 180\n",
      "\n",
      "Positive review:\n",
      " Dr. Ben McKenna (James Stewart) and Jo McKenna (Doris Day) travel to Morocco for a holiday where they meet a mysterious man named Louis Bernard (Daniel Gélin) on a bus.The next day this man is murdered, but before he dies he tells Ben a secret; an assassination will take place in London.The crooks kidnap the couple's son Hank (Christopher Olsen) making sure Ben won't reveal their plan to anybody.Alfred Hitchcock's The Man Who Knew Too Much (1956) is a very intense thriller.The acting is superb as it always is in Hitchcok's films.James Stewart is marvelous.Doris Day is a delightful person and actress and she gets to show her singing talents as well.The song Que Sera, Sera has an important part in the movie.This movie is a movie of many classic scenes.In the final scenes at the Albert Hall, done without dialogue, you can barely blink your eyes.This movie is fifty years old now.Time hasn't decreased its power in any way.\n",
      "\n",
      "Prediction: Positive, score: 0.9946415286035795\n",
      "\n",
      "Negative review:\n",
      " There were many 'spooky' westerns made in the 30s and early 40s, and although this has a strong beginning, it isn't one. Randy Bowers (John Wayne) stopping at a 'Halfway House' saloon, finds it to be full of dead bodies, the bartender's corpse draped over the bar holding a gun, eyes watching Randy from behind holes cut through eyes in a picture, and a player piano playing \"The Loveliest Night of the Year.\" <br /><br />It was the result of a robbery by the Marvin Black gang, to get Ed Rogers' $30,000. Randy is an investigator who \"works alone,\" who wastes little time in getting arrested, escaping (with Ed's daughter Sally's help) and literally landing in the midst of the Black gang's hideout behind a waterfall. It all moves along fairly quickly. Only one too many chases after Randy slow it down.<br /><br />We even get George Hayes, clean shaven and playing two parts-- Marvin Black, the vilest villain, as well as the Good Citizen, Matt the Mute, who communicates via handwritten messages. Having him play two opposite roles was a good idea, but the writing down of messages thing gets old real fast, even for him, as he finally gives up doing it near the end saying to Sally, \"Ah, I'm fed up with this!\" You can find George playing a vile, vile, double crossing villain in the serial \"The Lost City\" (1934).<br /><br />I think this is the only 'Lone Star' film in which the title relates to, or is mentioned in the film! Sally offers her hand to Randy and says, \"He's not alone anymore!\" Then cut to their arms around each other as they look out facing a lake. Sally's running off with Randy seems too abrupt and not sufficiently prepared for. Too much time spent on horseback escaping the sheriff.<br /><br />Not that bad considering everything, but not that great either. I'd really give it a 4 and a half.\n",
      "\n",
      "Prediction: Positive, score: 0.9808162253109265\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Negative\", \"Positive\"]\n",
    "#i=0\n",
    "i=7869 # This one the model gets wrong for positive but right for negative\n",
    "i = 180 # Positive correct, negative wrong\n",
    "#i = np.random.randint(len(test_neg_files))\n",
    "print(\"File number: {}\\n\".format(i))\n",
    "print(\"Positive review:\\n {}\".format(test_pos[i]))\n",
    "#print(\"\\nPrediction: {}\\n\".format(labels[y_pred[i]]))\n",
    "print(\"\\nPrediction: {0}, score: {1}\\n\".format(labels[y_pred[i]], y_proba[i][1]))\n",
    "\n",
    "print(\"Negative review:\\n {}\".format(test_neg[i]))\n",
    "print(\"\\nPrediction: {0}, score: {1}\".format(labels[y_pred[i+ len(test_pos_files)]], y_proba[i + len(test_pos_files)][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate via model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.9352565504\n",
      "cross entropy: 0.4155153606134335\n",
      "Error rate: 0.13327999999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC score: {}\".format(roc_auc_score(y_test, y_proba[:, 1])))\n",
    "print(\"cross entropy: {}\".format(log_loss(y_test, y_proba[:, 1])))\n",
    "print(\"Error rate: {}\".format(1-accuracy_score(y_test, y_pred)))\n",
    "bow_error_rate = 1-accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52771</th>\n",
       "      <td>1.596494</td>\n",
       "      <td>refreshing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71737</th>\n",
       "      <td>1.417661</td>\n",
       "      <td>wonderfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>1.334485</td>\n",
       "      <td>funniest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21086</th>\n",
       "      <td>1.330959</td>\n",
       "      <td>erotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21626</th>\n",
       "      <td>1.296048</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62709</th>\n",
       "      <td>1.278945</td>\n",
       "      <td>superb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9624</th>\n",
       "      <td>1.257285</td>\n",
       "      <td>carrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47652</th>\n",
       "      <td>1.253918</td>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62956</th>\n",
       "      <td>1.236689</td>\n",
       "      <td>surprisingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23670</th>\n",
       "      <td>1.232715</td>\n",
       "      <td>flawless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coefficient         words\n",
       "52771     1.596494    refreshing\n",
       "71737     1.417661   wonderfully\n",
       "25062     1.334485      funniest\n",
       "21086     1.330959        erotic\n",
       "21626     1.296048     excellent\n",
       "62709     1.278945        superb\n",
       "9624      1.257285        carrey\n",
       "47652     1.253918       perfect\n",
       "62956     1.236689  surprisingly\n",
       "23670     1.232715      flawless"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_me = False\n",
    "\n",
    "model_coefs = pd.DataFrame({'coefficient': clf.coef_.tolist()[0], 'words': vectorizer.get_feature_names()})\n",
    "pos_words = model_coefs.sort_values('coefficient', ascending=False).head(10)\n",
    "neg_words = model_coefs.sort_values('coefficient', ascending=False).tail(10)\n",
    "if save_me:\n",
    "    pos_words.to_csv(\"../slides/slide_data/positive_words.csv\", index=False)\n",
    "    neg_words.to_csv(\"../slides/slide_data/negative_words.csv\", index=False)\n",
    "pos_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36483</th>\n",
       "      <td>-1.487891</td>\n",
       "      <td>laughable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67907</th>\n",
       "      <td>-1.491771</td>\n",
       "      <td>unfunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>-1.512002</td>\n",
       "      <td>boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40886</th>\n",
       "      <td>-1.537860</td>\n",
       "      <td>mess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>-1.697967</td>\n",
       "      <td>awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36000</th>\n",
       "      <td>-1.759309</td>\n",
       "      <td>lacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49246</th>\n",
       "      <td>-1.806683</td>\n",
       "      <td>poorly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17396</th>\n",
       "      <td>-2.080688</td>\n",
       "      <td>disappointment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70473</th>\n",
       "      <td>-2.109686</td>\n",
       "      <td>waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71937</th>\n",
       "      <td>-2.183291</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coefficient           words\n",
       "36483    -1.487891       laughable\n",
       "67907    -1.491771         unfunny\n",
       "7358     -1.512002          boring\n",
       "40886    -1.537860            mess\n",
       "4139     -1.697967           awful\n",
       "36000    -1.759309           lacks\n",
       "49246    -1.806683          poorly\n",
       "17396    -2.080688  disappointment\n",
       "70473    -2.109686           waste\n",
       "71937    -2.183291           worst"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at scores for individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39687    0.932585\n",
       "Name: coefficient, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word = 'successful'\n",
    "# word = 'painful'\n",
    "# word = 'family'\n",
    "# word = 'inspired'\n",
    "# word = 'feelings'\n",
    "# word = 'human'\n",
    "# word = 'real'\n",
    "# word = 'discussions'\n",
    "# word = 'boring'\n",
    "# word = 'strong'\n",
    "# word = 'corpse'\n",
    "# word = 'piano'\n",
    "# word = 'gun'\n",
    "# word = 'fast'\n",
    "# word = 'horseback'\n",
    "# word = 'great'\n",
    "# word = 'bad'\n",
    "# word = 'loveliest'\n",
    "# word = 'marvin'\n",
    "# word = 'matt'\n",
    "# word = 'george'\n",
    "# word = 'predictable'\n",
    "# word = 'genius'\n",
    "# word = 'creativity'\n",
    "# word = 'excellent'\n",
    "word = 'marvel'\n",
    "\n",
    "model_coefs.coefficient[model_coefs.words == word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "* Rank the IMDB sentiments of the following nationalities: German, Slovenian, American, Italian.\n",
    "* What is the difference in sentiment score in use the singular or plural of the nationalities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec from GenSim\n",
    "Skip-gram, with text processing inspired by https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Actor',\n",
       "  'turned',\n",
       "  'director',\n",
       "  'Bill',\n",
       "  'Paxton',\n",
       "  'follows',\n",
       "  'up',\n",
       "  'his',\n",
       "  'promising',\n",
       "  'debut',\n",
       "  'the',\n",
       "  'Gothic',\n",
       "  'horror',\n",
       "  'Frailty',\n",
       "  'with',\n",
       "  'this',\n",
       "  'family',\n",
       "  'friendly',\n",
       "  'sports',\n",
       "  'drama',\n",
       "  'about',\n",
       "  'the',\n",
       "  'U',\n",
       "  'S',\n",
       "  'Open',\n",
       "  'where',\n",
       "  'a',\n",
       "  'young',\n",
       "  'American',\n",
       "  'caddy',\n",
       "  'rises',\n",
       "  'from',\n",
       "  'his',\n",
       "  'humble',\n",
       "  'background',\n",
       "  'to',\n",
       "  'play',\n",
       "  'against',\n",
       "  'his',\n",
       "  'Bristish',\n",
       "  'idol',\n",
       "  'in',\n",
       "  'what',\n",
       "  'was',\n",
       "  'dubbed',\n",
       "  'as',\n",
       "  'The',\n",
       "  'Greatest',\n",
       "  'Game',\n",
       "  'Ever',\n",
       "  'Played'],\n",
       " ['I',\n",
       "  'm',\n",
       "  'no',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'golf',\n",
       "  'and',\n",
       "  'these',\n",
       "  'scrappy',\n",
       "  'underdog',\n",
       "  'sports',\n",
       "  'flicks',\n",
       "  'are',\n",
       "  'a',\n",
       "  'dime',\n",
       "  'a',\n",
       "  'dozen',\n",
       "  'most',\n",
       "  'recently',\n",
       "  'done',\n",
       "  'to',\n",
       "  'grand',\n",
       "  'effect',\n",
       "  'with',\n",
       "  'Miracle',\n",
       "  'and',\n",
       "  'Cinderella',\n",
       "  'Man',\n",
       "  'but',\n",
       "  'some',\n",
       "  'how',\n",
       "  'this',\n",
       "  'film',\n",
       "  'was',\n",
       "  'enthralling',\n",
       "  'all',\n",
       "  'the',\n",
       "  'same',\n",
       "  'The',\n",
       "  'film',\n",
       "  'starts',\n",
       "  'with',\n",
       "  'some',\n",
       "  'creative',\n",
       "  'opening',\n",
       "  'credits',\n",
       "  'imagine',\n",
       "  'a',\n",
       "  'Disneyfied',\n",
       "  'version',\n",
       "  'of',\n",
       "  'the',\n",
       "  'animated',\n",
       "  'opening',\n",
       "  'credits',\n",
       "  'of',\n",
       "  'HBO',\n",
       "  's',\n",
       "  'Carnivale',\n",
       "  'and',\n",
       "  'Rome',\n",
       "  'but',\n",
       "  'lumbers',\n",
       "  'along',\n",
       "  'slowly',\n",
       "  'for',\n",
       "  'its',\n",
       "  'first',\n",
       "  'by',\n",
       "  'the',\n",
       "  'numbers',\n",
       "  'hour'],\n",
       " ['Once',\n",
       "  'the',\n",
       "  'action',\n",
       "  'moves',\n",
       "  'to',\n",
       "  'the',\n",
       "  'U',\n",
       "  'S',\n",
       "  'Open',\n",
       "  'things',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'very',\n",
       "  'well'],\n",
       " ['Paxton',\n",
       "  'does',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'job',\n",
       "  'and',\n",
       "  'shows',\n",
       "  'a',\n",
       "  'knack',\n",
       "  'for',\n",
       "  'effective',\n",
       "  'directorial',\n",
       "  'flourishes',\n",
       "  'I',\n",
       "  'loved',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'soaked',\n",
       "  'montage',\n",
       "  'of',\n",
       "  'the',\n",
       "  'action',\n",
       "  'on',\n",
       "  'day',\n",
       "  'two',\n",
       "  'of',\n",
       "  'the',\n",
       "  'open',\n",
       "  'that',\n",
       "  'propel',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'further',\n",
       "  'or',\n",
       "  'add',\n",
       "  'some',\n",
       "  'unexpected',\n",
       "  'psychological',\n",
       "  'depth',\n",
       "  'to',\n",
       "  'the',\n",
       "  'proceedings'],\n",
       " ['There',\n",
       "  's',\n",
       "  'some',\n",
       "  'compelling',\n",
       "  'character',\n",
       "  'development',\n",
       "  'when',\n",
       "  'the',\n",
       "  'British',\n",
       "  'Harry',\n",
       "  'Vardon',\n",
       "  'is',\n",
       "  'haunted',\n",
       "  'by',\n",
       "  'images',\n",
       "  'of',\n",
       "  'the',\n",
       "  'aristocrats',\n",
       "  'in',\n",
       "  'black',\n",
       "  'suits',\n",
       "  'and',\n",
       "  'top',\n",
       "  'hats',\n",
       "  'who',\n",
       "  'destroyed',\n",
       "  'his',\n",
       "  'family',\n",
       "  'cottage',\n",
       "  'as',\n",
       "  'a',\n",
       "  'child',\n",
       "  'to',\n",
       "  'make',\n",
       "  'way',\n",
       "  'for',\n",
       "  'a',\n",
       "  'golf',\n",
       "  'course'],\n",
       " ['He',\n",
       "  'also',\n",
       "  'does',\n",
       "  'a',\n",
       "  'good',\n",
       "  'job',\n",
       "  'of',\n",
       "  'visually',\n",
       "  'depicting',\n",
       "  'what',\n",
       "  'goes',\n",
       "  'on',\n",
       "  'in',\n",
       "  'the',\n",
       "  'players',\n",
       "  'heads',\n",
       "  'under',\n",
       "  'pressure'],\n",
       " ['Golf',\n",
       "  'a',\n",
       "  'painfully',\n",
       "  'boring',\n",
       "  'sport',\n",
       "  'is',\n",
       "  'brought',\n",
       "  'vividly',\n",
       "  'alive',\n",
       "  'here'],\n",
       " ['Credit',\n",
       "  'should',\n",
       "  'also',\n",
       "  'be',\n",
       "  'given',\n",
       "  'the',\n",
       "  'set',\n",
       "  'designers',\n",
       "  'and',\n",
       "  'costume',\n",
       "  'department',\n",
       "  'for',\n",
       "  'creating',\n",
       "  'an',\n",
       "  'engaging',\n",
       "  'period',\n",
       "  'piece',\n",
       "  'atmosphere',\n",
       "  'of',\n",
       "  'London',\n",
       "  'and',\n",
       "  'Boston',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'twentieth',\n",
       "  'century',\n",
       "  'You',\n",
       "  'know',\n",
       "  'how',\n",
       "  'this',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'end',\n",
       "  'not',\n",
       "  'only',\n",
       "  'because',\n",
       "  'it',\n",
       "  's',\n",
       "  'based',\n",
       "  'on',\n",
       "  'a',\n",
       "  'true',\n",
       "  'story',\n",
       "  'but',\n",
       "  'also',\n",
       "  'because',\n",
       "  'films',\n",
       "  'in',\n",
       "  'this',\n",
       "  'genre',\n",
       "  'follow',\n",
       "  'the',\n",
       "  'same',\n",
       "  'template',\n",
       "  'over',\n",
       "  'and',\n",
       "  'over',\n",
       "  'but',\n",
       "  'Paxton',\n",
       "  'puts',\n",
       "  'on',\n",
       "  'a',\n",
       "  'better',\n",
       "  'than',\n",
       "  'average',\n",
       "  'show',\n",
       "  'and',\n",
       "  'perhaps',\n",
       "  'indicates',\n",
       "  'more',\n",
       "  'talent',\n",
       "  'behind',\n",
       "  'the',\n",
       "  'camera',\n",
       "  'than',\n",
       "  'he',\n",
       "  'ever',\n",
       "  'had',\n",
       "  'in',\n",
       "  'front',\n",
       "  'of',\n",
       "  'it'],\n",
       " ['Despite',\n",
       "  'the',\n",
       "  'formulaic',\n",
       "  'nature',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'and',\n",
       "  'easy',\n",
       "  'film',\n",
       "  'to',\n",
       "  'root',\n",
       "  'for',\n",
       "  'that',\n",
       "  'deserves',\n",
       "  'to',\n",
       "  'find',\n",
       "  'an',\n",
       "  'audience']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "review = test_pos[1]\n",
    "# Turn review into list of sentences\n",
    "sentence_list = tokenizer.tokenize(review.strip())\n",
    "review_lol = [re.sub(\"[^a-zA-Z]\", \" \", soup(s, \"html5lib\").get_text()).split() for s in sentence_list]\n",
    "review_lol[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"\n",
    "    Sentence parser and iterator from file, modified from\n",
    "    https://rare-technologies.com/word2vec-tutorial/\n",
    "    \n",
    "    My modifications:\n",
    "        * Text cleansing\n",
    "        * Can accept list of directories\n",
    "    \"\"\"\n",
    "    def __init__(self, dirname, re_pattern = \"[^a-zA-Z]\"):\n",
    "        self.dirname = dirname\n",
    "        self.re_pattern = re_pattern\n",
    "        \n",
    "    def parse(self, raw_text):\n",
    "        text = soup(raw_text, \"html5lib\").get_text().lower()\n",
    "        return re.sub(self.re_pattern,\" \",text).split()\n",
    " \n",
    "    def __iter__(self):\n",
    "        if not isinstance(self.dirname, list):\n",
    "            self.dirname = [self.dirname]\n",
    "        for text_dir in self.dirname:\n",
    "            for fname in os.listdir(text_dir):\n",
    "                for line in open(os.path.join(text_dir, fname)):\n",
    "                    yield self.parse(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 8s, sys: 25.1 s, total: 19min 33s\n",
      "Wall time: 17min 46s\n",
      "47167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method KeyedVectors.word_vec of <gensim.models.keyedvectors.KeyedVectors object at 0x118469048>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = MySentences([train_pos_dir, train_neg_dir, train_unsup_dir])\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "%time model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=cores)\n",
    "wvs = model.wv\n",
    "print(len(wvs.vocab))\n",
    "wvs.word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMBD word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mathematician/cool': -0.11889151588692025,\n",
      " 'mathematician/insane': 0.098659624566295831,\n",
      " 'politician/cool': -0.10177807999433539,\n",
      " 'politician/crooked': 0.63820138175875307}\n"
     ]
    }
   ],
   "source": [
    "occupations = {}\n",
    "occupations['mathematician/cool'] = model.wv.similarity('mathematician', 'cool')\n",
    "occupations['mathematician/insane'] = model.wv.similarity('mathematician', 'insane')\n",
    "occupations['politician/cool'] = model.wv.similarity('politician', 'cool')\n",
    "occupations['politician/crooked'] = model.wv.similarity('politician', 'crooked')\n",
    "\n",
    "pprint(occupations)\n",
    "\n",
    "if save_me:\n",
    "    pd.Series(occupations).to_csv(\"../slides/slide_data/math_politics.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483251227961 0.539901505122 0.412879400084\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('germans', 'german'), \n",
    "      model.wv.similarity('americans', 'american'), \n",
    "      model.wv.similarity('italians', 'italian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare BoW to doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"../data/doc2vec_results.csv\"):\n",
    "    doc2vec_results = pd.read_csv(\"../data/doc2vec_results.csv\", header=None)#, index_col=0)\n",
    "    doc2vec_results.rename(columns={0: 'run', 1: \"error rate\"}, inplace=True)\n",
    "    doc2vec_results = doc2vec_results.append([{'run': 'BoW one-hot-encoding', 'error rate': bow_error_rate}])\n",
    "    if save_me:\n",
    "        doc2vec_results.sort_values('error rate').to_csv(\"../slides/slide_data/imdb_all_results.csv\", index=False)\n",
    "    print(doc2vec_results.sort_values('error rate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "* Give a brief explanation of what cross validation is, both as a way to measure model performance and a way to improve model generalization (i.e. predictive power on data the model has never seen).\n",
    "* `*` Use the cross validation version of logistic regression and test if the error rate decreases. Explain your results (general arguments enough)\n",
    "* `***` Use a neural network as a sentiment classifier on the BoW count vectors and compare performance, e.g. using Keras: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
